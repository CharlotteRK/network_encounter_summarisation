The summarisation tool discussed in this document provides a way to convert multiple formats of collected network data into a single output format. This includes tcpdump network traces and syslog files (as long as some details of the syslog format are given). A brief summary of whether or not each project requirement has been fulfilled is given below. Each requirement is repeated here in bold as it was given initially in section \ref{section:req} for convenience.
\\\\
\textbf{Reduce the quantity of data from the original dataset while maintaining any information identified as useful during research into how CRAWDAD datasets are used.}
A significant reduction in the size of files has been observed when the summarisation tool is run on network traces collected from the campus buildings of Dartmouth College. The data regarding reduction in file size is displayed in figure \ref{fig:size_red}. In section \ref{section:useage} a through summary has been presented showing research into which information from the CRAWDAD data sets is most useful in the context of network mobility and DTNs. Using this research a default summary format, a comma separated encounter summary, has been specified.
\\\\
\textbf{Produce summaries of the initial datasets that can be processed more efficiently than the original data.}
A rather trivial point to make here is that by removing the unnecessary data from the raw network traces and reducing their size, the time taken to extract more specific information from the summary in future must be achievable in stricter time and memory limits. This is due to all searching and sorting algorithms being limited by the size of the input they take. 
This tool also allows multiple formats of input data to be converted to an identical output format. By implementing this feature it is possible that data collected from multiple sources may be processed using the same downstream systems. This could save researchers from needing to develop multiple downstream tools to handle different formats of input in cases where mixed methods of data capture might be used.
\\\\
\textbf{Use a commonly found format to output my summaries and justify why this format is appropriate in context.}
The tool offers an option to output the summary in the format of an events trace as used by the ONE simulator. The ONE simulator has been mentioned several times throughout this document, initially in the context survey where it was identified as a commonly used tool for studies around network mobility and DTNs. Providing this events format gives a link to an already widely used system in the area of network mobility, therefore allowing the summaries to be used without new downstream processing systems being developed.
\\\\
\textbf{Allow multiple summaries to be merged (this may allow extension into distributed processing).}
The merging tool developed during this project is documented in detail in section \ref{section:merge}. It allows for any number of summary files to be placed in a directory and merged together into a single summary of the same format. The input summaries must have the same format, and in the case of the ONE simulator compatible  event traces, a modified format must be used to create the summaries before the merge. If the normal event trace format is used, information will be lost and the files may not merge as intended.
Although no extension into distributed processing has been implemented, this possible use of the merging tool has been considered in section \ref{section:merge}.  
\\\\
\textbf{Summarise at least two different formats of input data to create a standard output summary.}
Initially network traces output by tcpdump were the input format this project was developed for use on, however this was extended to also take syslog files as input. In the case of syslog input there must be a configuration file with certain variables (such as the location and format of device identifications) given in addition to the input file. In both cases, tcpdump and syslog, the same output formats are available. 
\\\\
Unfortunately, due to the time constraints of this project the following three medium/low priority requirements were not able to be implemented:
\begin{itemize}
\item \textbf{Allow a summary to be updated by the addition of a single data entry (this may allow extension into real-time processing).}
\item \textbf{Process datasets with an unknown input format.}
\item \textbf{Identify and report if a specific summary is likely to be unrepresentative of the input dataset due to aspects such as missing data or bias.}
\end{itemize}

Despite these features not being implemented yet, future development has been considered throughout the project. The summarisation tool has been specifically designed to be easy to build on in terms of new input formats; only a small part of the system would need to be re-written for a new input type to be added to the system.
If the input data is comprised of a large number of zipped files (such as pcap files segmented by time like those in the Dartmouth dataset) the unzipping process can take a significant amount of time. However, this is unavoidable as the files cannot be processed while compressed. A large amount of input data can also lead to the third stage of processing taking a significant time to run, however this is determined by the number of associations and access points in the data and not by the total input size. These time dependencies have been taken into account during the development and have been minimised where possible. Specifically, the ability to process small chunks of the input data individually and then merge their summaries after they have been output from the tool would provide a quicker execution in some cases.\\\\